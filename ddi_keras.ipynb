{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Input, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import NearMiss \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"nozeros2024.csv\")\n",
    "feature_cols=[\"rel1_ADMINISTERED_TO\", \"rel1_AFFECTS\", \"rel1_ASSOCIATED_WITH\", \"rel1_AUGMENTS\", \"rel1_CAUSES\", \"rel1_COEXISTS_WITH\", \"rel1_compared_with\", \"rel1_COMPLICATES\", \"rel1_CONVERTS_TO\", \"rel1_DIAGNOSES\", \"rel1_different_from\", \"rel1_different_than\", \"rel1_DISRUPTS\", \"rel1_higher_than\", \"rel1_INHIBITS\", \"rel1_INTERACTS_WITH\", \"rel1_IS_A\", \"rel1_ISA\", \"rel1_LOCATION_OF\", \"rel1_lower_than\", \"rel1_MANIFESTATION_OF\", \"rel1_METHOD_OF\", \"rel1_OCCURS_IN\", \"rel1_PART_OF\", \"rel1_PRECEDES\", \"rel1_PREDISPOSES\", \"rel1_PREVENTS\", \"rel1_PROCESS_OF\", \"rel1_PRODUCES\", \"rel1_same_as\", \"rel1_STIMULATES\", \"rel1_TREATS\", \"rel1_USES\", \"rel1_MENTIONED_IN\", \"rel1_HAS_MESH\", \"rel2_ADMINISTERED_TO\", \"rel2_AFFECTS\", \"rel2_ASSOCIATED_WITH\", \"rel2_AUGMENTS\", \"rel2_CAUSES\", \"rel2_COEXISTS_WITH\", \"rel2_compared_with\", \"rel2_COMPLICATES\", \"rel2_CONVERTS_TO\", \"rel2_DIAGNOSES\", \"rel2_different_from\", \"rel2_different_than\", \"rel2_DISRUPTS\", \"rel2_higher_than\", \"rel2_INHIBITS\", \"rel2_INTERACTS_WITH\", \"rel2_IS_A\", \"rel2_ISA\", \"rel2_LOCATION_OF\", \"rel2_lower_than\", \"rel2_MANIFESTATION_OF\", \"rel2_METHOD_OF\", \"rel2_OCCURS_IN\", \"rel2_PART_OF\", \"rel2_PRECEDES\", \"rel2_PREDISPOSES\", \"rel2_PREVENTS\", \"rel2_PROCESS_OF\", \"rel2_PRODUCES\", \"rel2_same_as\", \"rel2_STIMULATES\", \"rel2_TREATS\", \"rel2_USES\", \"rel2_MENTIONED_IN\", \"rel2_HAS_MESH\", \"rel3_ADMINISTERED_TO\", \"rel3_AFFECTS\", \"rel3_ASSOCIATED_WITH\", \"rel3_AUGMENTS\", \"rel3_CAUSES\", \"rel3_COEXISTS_WITH\", \"rel3_compared_with\", \"rel3_COMPLICATES\", \"rel3_CONVERTS_TO\", \"rel3_DIAGNOSES\", \"rel3_different_from\", \"rel3_different_than\", \"rel3_DISRUPTS\", \"rel3_higher_than\", \"rel3_INHIBITS\", \"rel3_INTERACTS_WITH\", \"rel3_IS_A\", \"rel3_ISA\", \"rel3_LOCATION_OF\", \"rel3_lower_than\", \"rel3_MANIFESTATION_OF\", \"rel3_METHOD_OF\", \"rel3_OCCURS_IN\", \"rel3_PART_OF\", \"rel3_PRECEDES\", \"rel3_PREDISPOSES\", \"rel3_PREVENTS\", \"rel3_PROCESS_OF\", \"rel3_PRODUCES\", \"rel3_same_as\", \"rel3_STIMULATES\", \"rel3_TREATS\", \"rel3_USES\", \"rel3_MENTIONED_IN\", \"rel3_HAS_MESH\"]\n",
    "\n",
    "data['INTERACTS'].hist()\n",
    "X=data[feature_cols]\n",
    "y=data[\"INTERACTS\"]\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\n",
    "print(X_train)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(X_train.shape[1],)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(BatchNormalization())\n",
    "                           \n",
    "model.add(Dropout(0.1))   # Dropout helps protect the model from memorizing or \"overfitting\" the training data\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy',metrics=tf.keras.metrics.Recall())\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=128, epochs=100, verbose=2,\n",
    "          validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
